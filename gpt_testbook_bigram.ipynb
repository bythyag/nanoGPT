{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrz0pNBugopH8M+Ct4d03l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bythyag/nanoGPT/blob/main/gpt_testbook_bigram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvtGAScGec9h",
        "outputId": "782d6681-b751-4581-8889-15a3fe9e43c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-26 08:03:46--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-03-26 08:03:46 (26.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# in the example karpathy used shakespeare dataset so we will be looking at that only here\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "iwr50blhepPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IqJO_Vces5d",
        "outputId": "28afec32-2051-4fce-ba3a-bfd72b1fc0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first 1000 characters\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZJKFFqOexXE",
        "outputId": "f419df5c-fcca-4ef1-c9c2-835ec75fb77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since we are training out gpt on next character predicition, we find all the unique characters that occur in this text\n",
        "\n",
        "#in set no duplicates are allowed, so this means we pick up all the unique characters throught out set() function\n",
        "print(f\"set of unique characters: {set(text)}\")\n",
        "\n",
        "# then we convert the set to a list\n",
        "print(f\"list of unique characters: {list(set(text))}\")\n",
        "\n",
        "# then we sort the list of chars and find the total unique characters in the chars\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"sorted list of unique characters: {chars}\")\n",
        "print(f\"unique characters in the text {''.join(chars)}\")\n",
        "print(f\"vocab size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbcKWWmDfG4E",
        "outputId": "49f6c3e7-20ae-4ab5-a09b-fdf9f17c0abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "set of unique characters: {'t', '\\n', '&', 'H', 'P', 'c', ':', '$', '!', 'z', 'B', 'b', 'e', 'm', 'F', 'J', 'I', 'O', 'M', 'j', 'T', 'U', 'g', 'R', 'Z', '3', ',', '-', 'p', \"'\", 'a', '?', 's', 'o', 'V', 'r', 'y', 'k', 'i', 'A', 'S', 'C', 'x', 'd', 'n', 'E', 'L', 'q', 'G', 'K', '.', 'Q', 'u', 'f', 'h', 'v', ';', 'W', ' ', 'D', 'Y', 'l', 'w', 'X', 'N'}\n",
            "list of unique characters: ['t', '\\n', '&', 'H', 'P', 'c', ':', '$', '!', 'z', 'B', 'b', 'e', 'm', 'F', 'J', 'I', 'O', 'M', 'j', 'T', 'U', 'g', 'R', 'Z', '3', ',', '-', 'p', \"'\", 'a', '?', 's', 'o', 'V', 'r', 'y', 'k', 'i', 'A', 'S', 'C', 'x', 'd', 'n', 'E', 'L', 'q', 'G', 'K', '.', 'Q', 'u', 'f', 'h', 'v', ';', 'W', ' ', 'D', 'Y', 'l', 'w', 'X', 'N']\n",
            "sorted list of unique characters: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "unique characters in the text \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers in a dictionary\n",
        "# eg: in stoi ch is mapped to i with corresponding character and its index value as we had derived from above\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }"
      ],
      "metadata": {
        "id": "VDAa-U8zfUrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stoi)\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzQazXyDh3sm",
        "outputId": "9d4b6157-4926-42a1-abca-bdb309966dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder-decoder functions\n",
        "\n",
        "# encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "# decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"Hello, How are you?\"))\n",
        "print(decode(encode(\"Hello, How are you?\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHSsd9MXh8fl",
        "outputId": "31283676-0cef-4dfc-948f-648d23ee7c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 50, 50, 53, 6, 1, 20, 53, 61, 1, 39, 56, 43, 1, 63, 53, 59, 12]\n",
            "Hello, How are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will now encode the entire text dataset and save as pytorch tensor (torch.Tensor)\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "JkESzI9eok9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at the shape and data type of the tensor we have created\n",
        "print(data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InR6d1lipJGm",
        "outputId": "5877dfa4-2c0b-40d0-de93-d150dc3dd61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's checkout the first 100 tensor values\n",
        "# the 100 characters we looked at earier will to the GPT look like this\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkPwPMjcpNJT",
        "outputId": "60f83130-a372-42ac-f7bf-082269e62967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spilt the dataset into testing set and training set\n",
        "# first 80% will be train, rest val\n",
        "n = int(0.8*len(data))\n",
        "\n",
        "#test set\n",
        "train_data = data[:n]\n",
        "\n",
        "#training set\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "pKP2QtgipaZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# length of data\n",
        "print(f\"length of the training data: {len(train_data)}\")\n",
        "print(f\"length of the test data: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOgquQRWruHy",
        "outputId": "9a64ffa1-75d6-4516-f86b-b4925e0cb48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the training data: 892315\n",
            "length of the test data: 223079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for reproduce we use torch manual_seed\n",
        "torch.manual_seed(1111)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OogHXa96p8Fq",
        "outputId": "523c6678-a510-4346-85a2-66780baf600f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c513216e250>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4 # total independent sequence we can process in parallel\n",
        "block_size = 8 # maximum context length\n",
        "\n",
        "# generate a small batch of data of inputs x and targets y\n",
        "# i.e given first x characters and the corresponding target x+1 character\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "jOry-0NwqWKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating our splits\n",
        "x_train, y_train = get_batch('train')\n",
        "\n",
        "print(f\"input: {x_train}\")\n",
        "print(f\"target: {y_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-QYKk0gqlCZ",
        "outputId": "5b1c9018-ad11-4303-df53-394343b044ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tensor([[47, 53, 52,  8,  0, 27,  1, 58],\n",
            "        [ 1, 61, 46, 47, 58, 46, 43, 56],\n",
            "        [ 1, 39, 54, 54, 43, 39, 56,  1],\n",
            "        [41, 53, 51, 54, 39, 52, 47, 43]])\n",
            "target: tensor([[53, 52,  8,  0, 27,  1, 58, 47],\n",
            "        [61, 46, 47, 58, 46, 43, 56,  1],\n",
            "        [39, 54, 54, 43, 39, 56,  1, 53],\n",
            "        [53, 51, 54, 39, 52, 47, 43, 42]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for visualisation:\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = x_train[b, :t+1]\n",
        "        target = y_train[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWwMNzFAt1F6",
        "outputId": "9a9fea9d-6d27-4ce5-9fe3-0a64202c2771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is [47] the target: 53\n",
            "when input is [47, 53] the target: 52\n",
            "when input is [47, 53, 52] the target: 8\n",
            "when input is [47, 53, 52, 8] the target: 0\n",
            "when input is [47, 53, 52, 8, 0] the target: 27\n",
            "when input is [47, 53, 52, 8, 0, 27] the target: 1\n",
            "when input is [47, 53, 52, 8, 0, 27, 1] the target: 58\n",
            "when input is [47, 53, 52, 8, 0, 27, 1, 58] the target: 47\n",
            "---\n",
            "when input is [1] the target: 61\n",
            "when input is [1, 61] the target: 46\n",
            "when input is [1, 61, 46] the target: 47\n",
            "when input is [1, 61, 46, 47] the target: 58\n",
            "when input is [1, 61, 46, 47, 58] the target: 46\n",
            "when input is [1, 61, 46, 47, 58, 46] the target: 43\n",
            "when input is [1, 61, 46, 47, 58, 46, 43] the target: 56\n",
            "when input is [1, 61, 46, 47, 58, 46, 43, 56] the target: 1\n",
            "---\n",
            "when input is [1] the target: 39\n",
            "when input is [1, 39] the target: 54\n",
            "when input is [1, 39, 54] the target: 54\n",
            "when input is [1, 39, 54, 54] the target: 43\n",
            "when input is [1, 39, 54, 54, 43] the target: 39\n",
            "when input is [1, 39, 54, 54, 43, 39] the target: 56\n",
            "when input is [1, 39, 54, 54, 43, 39, 56] the target: 1\n",
            "when input is [1, 39, 54, 54, 43, 39, 56, 1] the target: 53\n",
            "---\n",
            "when input is [41] the target: 53\n",
            "when input is [41, 53] the target: 51\n",
            "when input is [41, 53, 51] the target: 54\n",
            "when input is [41, 53, 51, 54] the target: 39\n",
            "when input is [41, 53, 51, 54, 39] the target: 52\n",
            "when input is [41, 53, 51, 54, 39, 52] the target: 47\n",
            "when input is [41, 53, 51, 54, 39, 52, 47] the target: 43\n",
            "when input is [41, 53, 51, 54, 39, 52, 47, 43] the target: 42\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets write our bigram model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1111)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPzps2K4ufiw",
        "outputId": "a9c99afa-d58c-4642-bef6-2a0e1c6e9f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c513216e250>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple Bigram-based Language Model that predicts the next token\n",
        "    based on the current token using an embedding lookup table.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Token embedding layer (lookup table for next-token logits)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # Linear layer to allow transformation of logits\n",
        "        self.output_layer = nn.Linear(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_indices, target_indices=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the language model.\n",
        "\n",
        "        Args:\n",
        "        - input_indices (Tensor): Tensor of shape (B, T), where B is batch size, and T is sequence length (block size = 8 in our case).\n",
        "        - target_indices (Tensor, optional): Ground truth token indices for computing loss (same shape as input_indices).\n",
        "\n",
        "        Returns:\n",
        "        - logits (Tensor): Predicted logits for the next token (B, T, vocab_size)\n",
        "        - loss (Tensor, optional): Cross-entropy loss if target_indices is provided.\n",
        "        \"\"\"\n",
        "        # Get token embeddings for the input indices\n",
        "        logits = self.token_embedding(input_indices)  # Shape: (B, T, vocab_size)\n",
        "        logits = self.output_layer(logits)  # Shape remains (B, T, vocab_size)\n",
        "\n",
        "        # Compute loss only if target tokens are provided\n",
        "        loss = None\n",
        "        if target_indices is not None:\n",
        "            B, T, V = logits.shape  # Batch size, Sequence length, Vocabulary size\n",
        "            logits = logits.view(B * T, V)  # Flatten logits for loss computation\n",
        "            target_indices = target_indices.view(B * T)  # Flatten targets\n",
        "            loss = F.cross_entropy(logits, target_indices)  # Compute loss\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, input_indices, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Generates new tokens based on the input sequence.\n",
        "\n",
        "        Args:\n",
        "        - input_indices (Tensor): Initial sequence of token indices (B, T).\n",
        "        - max_new_tokens (int): Number of tokens to generate.\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Generated sequence including the input indices.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get model predictions\n",
        "            logits, _ = self(input_indices)  # (B, T, vocab_size)\n",
        "\n",
        "            # Focus only on the last timestep's logits\n",
        "            last_logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "\n",
        "            # Convert logits to probabilities\n",
        "            probabilities = F.softmax(last_logits, dim=-1)  # (B, vocab_size)\n",
        "\n",
        "            # Sample a token from the probability distribution\n",
        "            next_token = torch.multinomial(probabilities, num_samples=1)  # (B, 1)\n",
        "\n",
        "            # Append the sampled token to the sequence\n",
        "            input_indices = torch.cat((input_indices, next_token), dim=1)  # (B, T+1)\n",
        "\n",
        "        return input_indices\n"
      ],
      "metadata": {
        "id": "zDE2wac-uqkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "logits, loss = model(x_train, y_train)\n",
        "\n",
        "print(f\"logits shape: {logits.shape}\")\n",
        "print(f\"loss: {loss}\")\n",
        "\n",
        "print(decode(model.generate(input_indices = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2AAFgaxxSfV",
        "outputId": "74b6e1a6-711e-4d42-d239-1150993dabc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shape: torch.Size([32, 65])\n",
            "loss: 4.379518508911133\n",
            "\n",
            "T,oRWeF'tEJvqxL uGqkPWqNMyHRdcmc,f.pdMJcnMts\n",
            ",L3vOrf&Wwtw:RF.MyBA'!fnWgKeFsg,wq;yHjOlDh\n",
            "Yji;CYqfKCR'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "JiSUE-I9yRyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100000000):\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # check loss over interval\n",
        "    if steps % 10000 == 0:\n",
        "        print(f\"step: {steps} loss: {loss.item()}\")\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "t6x3SOo2ytjC",
        "outputId": "9fffe07b-236b-4604-d6b6-d49352a30a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0 loss: 2.4813029766082764\n",
            "step: 10000 loss: 2.4824483394622803\n",
            "step: 20000 loss: 2.5528903007507324\n",
            "step: 30000 loss: 2.4531195163726807\n",
            "step: 40000 loss: 2.4355146884918213\n",
            "step: 50000 loss: 2.5187554359436035\n",
            "step: 60000 loss: 2.4074172973632812\n",
            "step: 70000 loss: 2.4454853534698486\n",
            "step: 80000 loss: 2.5549559593200684\n",
            "step: 90000 loss: 2.5019357204437256\n",
            "step: 100000 loss: 2.5201737880706787\n",
            "step: 110000 loss: 2.4098129272460938\n",
            "step: 120000 loss: 2.4527106285095215\n",
            "step: 130000 loss: 2.414992570877075\n",
            "step: 140000 loss: 2.3709492683410645\n",
            "step: 150000 loss: 2.4506192207336426\n",
            "step: 160000 loss: 2.4866151809692383\n",
            "step: 170000 loss: 2.4811582565307617\n",
            "step: 180000 loss: 2.4354517459869385\n",
            "step: 190000 loss: 2.288470506668091\n",
            "step: 200000 loss: 2.4022574424743652\n",
            "step: 210000 loss: 2.328784465789795\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-3a45e4f7eca8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(input_indices = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKp-RSYhzKnp",
        "outputId": "d9d411f3-dcc0-456a-9d50-4d75c52686de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "YOUR:\n",
            "\n",
            "KERGope cavebral intispr omergdet d,\n",
            "\n",
            "W:\n",
            "HEn thaske e\n",
            "Fistoular ceerth aked, govis wimpay see\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## some thoughts\n",
        "1. loss is stuck at ~ 2.4 after several training and output doesn't make much sense\n",
        "2. i asked claude for suggestion and it added attention layer which i don't want on this bigram model\n",
        "3. further asking to improve on the architecture, it suggested to add positional embedding in the layer, layer normalisation, l2 regularisation, and drop out during the training process"
      ],
      "metadata": {
        "id": "ZypBH67G076j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eauib36p1rg5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}